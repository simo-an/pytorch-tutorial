{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导与计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 y = w1*x^2 + w2*x + w3\n",
    "\n",
    "import torch\n",
    "\n",
    "w1 = torch.tensor(1., requires_grad=True)\n",
    "w2 = torch.tensor(2., requires_grad=True)\n",
    "w3 = torch.tensor(3., requires_grad=True)\n",
    "x = torch.tensor(4.)\n",
    "\n",
    "print(w1, w2, w3, x)\n",
    "\n",
    "y = w1 * (x ** 2) + w2 * x + w3\n",
    "\n",
    "y.backward()\n",
    "\n",
    "print(w1.grad) # x ^ 2 = 16\n",
    "print(w2.grad) # x = 4\n",
    "print(w3.grad) # 1\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.requires_grad)\n",
    "with torch.no_grad():\n",
    "    z = w1 * x + w2\n",
    "    print(x.requires_grad) # False\n",
    "\n",
    "h = y.detach()\n",
    "print(h.requires_grad) # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络工具箱 torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "# Linear\n",
    "class Linear(nn.Module):\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "    bias: Tensor\n",
    "    def __init__(self, \n",
    "        in_features: int, out_features: int,\n",
    "        bias: bool = True, device=None, dtype=None\n",
    "    ) -> None:\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), device=device, dtype=dtype))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features, device=device, dtype=dtype))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        \n",
    "        self.init_parameters()\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        if self.bias is None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "        \n",
    "    def forward(self, input: Tensor):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "# Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(nn.Module):\n",
    "    inplace: bool\n",
    "    def __init__(self, inplace: bool = False) -> None:\n",
    "        super(Relu, self).__init__()\n",
    "        self.inplace = inplace\n",
    "    \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.relu(input, inplace=self.inplace)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'inplace=True' if self.inplace else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class _Loss(nn.Module):\n",
    "    reduction: str\n",
    "\n",
    "    def __init__(self, size_avg=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_avg is not None or reduce is not None:\n",
    "            self.reduction = nn._reduction.legacy_get_string(size_avg, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class _WeightedLoss(_Loss):\n",
    "    weight: Optional[Tensor]\n",
    "    def __init__(self, \n",
    "        weight: Optional[Tensor] = None, \n",
    "        size_avg=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_avg=size_avg, reduce=reduce, reduction=reduction)\n",
    "        self.register_buffer('weight', weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(_WeightedLoss):\n",
    "    __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
    "    ignore_index: int\n",
    "    label_smoothing: float\n",
    "\n",
    "    def __init__(self, \n",
    "        weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "        reduce=None, reduction: str = 'mean', label_smoothing: float = 0.0) -> None:\n",
    "        \n",
    "        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)\n",
    "        self.ignore_index = ignore_index\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        return torch.nn.functional.cross_entropy(input, target, weight=self.weight,\n",
    "                               ignore_index=self.ignore_index, reduction=self.reduction,\n",
    "                               label_smoothing=self.label_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SmoothL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothL1Loss(_Loss):\n",
    "    def __init__(self, size_avg=None, reduce=None, reduction: str = 'mean', beta: float = 1.0) -> None:\n",
    "        super(SmoothL1Loss, self).__init__(size_avg=size_avg, reduce=reduce, reduction=reduction)\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
    "        diff = torch.abs(input - target)\n",
    "        cond = diff < self.beta\n",
    "\n",
    "        loss = torch.where(cond, 0.5 * (diff**2)/self.beta, diff - 0.5 * self.beta)\n",
    "\n",
    "        return torch.sum(loss) if self.reduction == 'sum' else torch.mean(loss)\n",
    "\n",
    "x = torch.randint(5, (2, 1, 2, 2)).float()\n",
    "y = torch.randint(5, (2, 1, 2, 2)).float()\n",
    "\n",
    "crit = SmoothL1Loss(reduction='sum')\n",
    "crit_t = nn.SmoothL1Loss(reduction='sum')\n",
    "\n",
    "print(crit(x, y))\n",
    "print(crit_t(x, y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器 —— SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim._functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class SGD(optim.Optimizer):\n",
    "    def __init__(self, \n",
    "        params, lr: float = 0.001, weight_decay=0\n",
    "    ) -> None:\n",
    "        default = dict(\n",
    "            lr=lr, momentum=0, dampening=0,\n",
    "            weight_decay=weight_decay, nesterov=False\n",
    "        )\n",
    "        super().__init__(params, default)\n",
    "    \n",
    "    def __setstate__(self, state: dict) -> None:\n",
    "        super().__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            d_p_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            lr = group['lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    d_p_list.append(p.grad)\n",
    "            \n",
    "            F.sgd(params_with_grad,d_p_list, [], \n",
    "                    weight_decay=weight_decay,lr=lr, momentum=0,\n",
    "                    dampening=0, nesterov=False\n",
    "            )\n",
    "\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels) -> None:\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_channels, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP(28*28, 10)\n",
    "\n",
    "optimizer = SGD(params=mlp_model.parameters(), lr=0.05)\n",
    "data = torch.randn(10, 28*28)\n",
    "\n",
    "\n",
    "\n",
    "label = torch.Tensor([1, 0, 4, 7, 9, 3, 4, 5, 3, 6]).long()\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    output = mlp_model(data)\n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    optimizer.zero_grad() # 清空梯度\n",
    "    loss.backward()       # 反向传播\n",
    "    optimizer.step()      # 梯度更新\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision as TV\n",
    "\n",
    "vgg16 = TV.models.vgg16()\n",
    "\n",
    "print(len(vgg16.features))\n",
    "print(len(vgg16.classifier))\n",
    "print(vgg16.classifier[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import visdom\n",
    "\n",
    "vis = visdom.Visdom(env='first')\n",
    "vis.text('Hello Visdom', win='text1')\n",
    "vis.text('Hello Pytorch', win='text1', append=True)\n",
    "\n",
    "# 绘制 y = -x^2 + 20x + 1 \n",
    "for x in range(20):\n",
    "    vis.line(\n",
    "        X=torch.FloatTensor([x]), Y=torch.FloatTensor([-x ** 2 + 20 * x + 1]),\n",
    "        opts={'title': 'y = -x^2 + 20x + 1'},\n",
    "        win='loss',\n",
    "        update='append'\n",
    "    )\n",
    "\n",
    "vis.image(torch.randint(1, 255, (3, 255, 255)).float(), win='random image')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
