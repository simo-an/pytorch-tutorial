{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import FrEIA.framework as Ff\n",
    "import FrEIA.modules as Fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 样例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = Ff.InputNode(4, name='Input1')\n",
    "input2 = Ff.InputNode(2, name='Input2')\n",
    "cond = Ff.ConditionNode(3, name='Condition')\n",
    "\n",
    "def subnet_fc(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, 16), nn.ReLU(),\n",
    "        nn.Linear(16, out_dim)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4., 6., 8.]]) tensor([[2., 4., 6., nan]], grad_fn=<IndexBackward0>)\n",
      "tensor([[3., 6.]]) tensor([[nan, nan]], grad_fn=<SplitWithSizesBackward0>)\n",
      "tensor([nan], grad_fn=<AddBackward0>) tensor([nan], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 定义网络结构\n",
    "perm = Ff.Node(inputs=input1, module_type=Fm.PermuteRandom, module_args={}, name='PermuteRandom')\n",
    "split1 = Ff.Node(inputs=perm, module_type=Fm.Split, module_args={}, name='Split 1')\n",
    "split2 = Ff.Node(inputs=split1.out1, module_type=Fm.Split, module_args={}, name='Split 2')\n",
    "actnorm = Ff.Node(inputs=split2.out1, module_type=Fm.ActNorm, module_args={}, name='ActNorm')\n",
    "\n",
    "concat1 = Ff.Node(inputs=[actnorm.out0, input2.out0], module_type=Fm.Concat, module_args={}, name='Concat1')\n",
    "\n",
    "affine = Ff.Node(\n",
    "    inputs=concat1, \n",
    "    module_type=Fm.AffineCouplingOneSided,\n",
    "    module_args={ 'subnet_constructor': subnet_fc },\n",
    "    conditions=cond,\n",
    "    name='AffineCouplingOneSided'\n",
    ")\n",
    "\n",
    "concat2 = Ff.Node(inputs=[split2.out0, affine.out0], module_type=Fm.Concat, module_args={}, name='Concat2')\n",
    "\n",
    "# 得到两个输出\n",
    "output1 = Ff.OutputNode(split1.out0, name='Output1')\n",
    "output2 = Ff.OutputNode(concat2, name='Output2')\n",
    "\n",
    "INN_Model = Ff.GraphINN(node_list=[\n",
    "    input1, input2, cond,\n",
    "    perm, split1, split2,\n",
    "    actnorm, concat1, affine, concat2,\n",
    "    output1, output2\n",
    "])\n",
    "\n",
    "in1, in2, c = torch.tensor([[2., 4., 6., 8.]]), torch.tensor([[3., 6.]]), torch.tensor([[2., 5., 7.]])\n",
    "\n",
    "(z1, z2), J = INN_Model([in1, in2], c=c)\n",
    "\n",
    "(in1_inv, in2_inv), J_inv = INN_Model([z1, z2], c=c, rev=True)\n",
    "\n",
    "print(in1, in1_inv)\n",
    "print(in2, in2_inv)\n",
    "print(J, J_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subnet_fc(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "        nn.Linear(256, out_dim)\n",
    "    )\n",
    "\n",
    "def subnet_conv(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_dim, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, out_dim, kernel_size=3, padding=1)\n",
    "    )\n",
    "\n",
    "def subnet_conv_1x1(in_dim, out_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_dim, 256, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(256, out_dim, kernel_size=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二维的可逆神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceINN(\n",
      "  (module_list): ModuleList(\n",
      "    (0): AllInOneBlock(\n",
      "      (softplus): Softplus(beta=0.5, threshold=20)\n",
      "      (subnet): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): AllInOneBlock(\n",
      "      (softplus): Softplus(beta=0.5, threshold=20)\n",
      "      (subnet): Sequential(\n",
      "        (0): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inn = Ff.SequenceINN(2)\n",
    "\n",
    "for k in range(2):\n",
    "    inn.append(Fm.AllInOneBlock, subnet_constructor=subnet_fc, permute_soft=True)\n",
    "\n",
    "print(inn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional INN 在Mnist数据集上实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FrEIA.framework as FF\n",
    "import FrEIA.modules as FM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceINN(\n",
      "  (module_list): ModuleList(\n",
      "    (0): AllInOneBlock(\n",
      "      (softplus): Softplus(beta=0.5, threshold=20)\n",
      "      (subnet): Sequential(\n",
      "        (0): Conv2d(402, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 784, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (1): AllInOneBlock(\n",
      "      (softplus): Softplus(beta=0.5, threshold=20)\n",
      "      (subnet): Sequential(\n",
      "        (0): Conv2d(402, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 784, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cinn = FF.SequenceINN(28 * 28)\n",
    "for k in range(2):\n",
    "    cinn.append(FM.AllInOneBlock, cond=0, cond_shape=(10,), subnet_constructor=subnet_conv_1x1)\n",
    "\n",
    "print(cinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional INN For CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [FF.InputNode(3, 32, 32, name='Input')]\n",
    "image_len = 3 * 32 * 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 较高分辨率卷积部分\n",
    "for k in range(4):\n",
    "    nodes.append(FF.Node(\n",
    "        inputs=nodes[-1], # 取出最后一个结点\n",
    "        module_type=FM.GLOWCouplingBlock,\n",
    "        module_args={'subnet_constructor': subnet_conv, 'clamp': 1.2},\n",
    "        name=f'HigherConv-{k}'\n",
    "    ))\n",
    "\n",
    "# 较低分辨率卷积部分\n",
    "for k in range(12):\n",
    "    subnet = subnet_conv_1x1 if k % 2 == 0 else subnet_conv\n",
    "\n",
    "    nodes.append(FF.Node(nodes[-1],FM.GLOWCouplingBlock, module_args={'subnet_constructor': subnet, 'clamp': 1.2}, name=f'LowerConv-{k}'))\n",
    "    nodes.append(FF.Node(nodes[-1],FM.PermuteRandom, module_args={'seed': k}, name=f'LowerPermute-{k}'))\n",
    "\n",
    "# 全连接\n",
    "nodes.append(FF.Node(nodes[-1], FM.Flatten, {}, name='Flatten'))\n",
    "split_node = FF.Node(\n",
    "    nodes[-1], \n",
    "    FM.Split, \n",
    "    {'section_sizes': (image_len // 4, 3 * image_len // 4), 'dim':0},\n",
    "    name='Split'\n",
    ")\n",
    "\n",
    "nodes.append(split_node)\n",
    "\n",
    "for k in range(12):\n",
    "    nodes.append(FF.Node(nodes[-1], FM.GLOWCouplingBlock, {'subnet_constructor': subnet_fc, 'clamp': 2.0}, name=f'FC_{k}'))\n",
    "    nodes.append(FF.Node(nodes[-1], FM.PermuteRandom, {'seed': k }, name=f'FC_Permute{k}'))\n",
    "\n",
    "nodes.append(FF.Node([nodes[-1].out0, split_node.out1], FM.Concat1d, {'dim': 0}, name='concat'))\n",
    "nodes.append(FF.OutputNode(nodes[-1], name='output'))\n",
    "\n",
    "conv_inn = FF.GraphINN(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编写自定义可逆操作\n",
    "\n",
    "自定义的可逆模块可以写成`FM.InvertibleModule`基类的扩展。有关需求的详细信息，请参阅这类文档。\n",
    "\n",
    "下面是两个简单的例子，它们说明了自定义模块的定义和使用，并且可以作为基本模板使用。第一种方法是将输入张量的每个维数乘以1或2，以随机但固定的方式选择。第二个是一个条件操作，它接受两个输入，如果条件为正，则交换它们，否则不做任何操作。\n",
    "\n",
    "注:\n",
    "- `FM.InvertibleModule`必须用`in_DIM`参数初始化，如果有条件输入，则可以使用`cond_dim`参数初始化。\n",
    "- `forward()`应该返回一个包含输出的元组(即使只有一个)，当`jac=True`时返回带有额外的`Log_Jacobian_Det`项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义\n",
    "class FixedRandomElementwiseMultiply(FM.InvertibleModule):\n",
    "    def __init__(self, dims_in) -> None:\n",
    "        super().__init__(dims_in)\n",
    "        self.random_factor = torch.randint(1, 3, size=(1, dims_in[0][0]))\n",
    "    \n",
    "    def forward(self, x, rev=False, jac=True):\n",
    "        x = x[0]\n",
    "        if not rev:\n",
    "            # 前向操作\n",
    "            x = x * self.random_factor\n",
    "            log_jacobian_det = self.random_factor.float().log().sum()\n",
    "        else:\n",
    "            # 后项操作\n",
    "            x = x / self.random_factor\n",
    "            log_jacobian_det = -self.random_factor.float().log().sum()\n",
    "\n",
    "        return (x,), log_jacobian_det\n",
    "    \n",
    "    def output_dims(self, dims_in):\n",
    "        return dims_in\n",
    "\n",
    "class ConditionalSwap(FM.InvertibleModule):\n",
    "    def __init__(self, dims_in, dims_c) -> None:\n",
    "        super().__init__(dims_in=dims_in, dims_c=dims_c)\n",
    "        \n",
    "    def forward(self, x, c, rev=False, jac=True): # c means condition\n",
    "        x1, x2 = x\n",
    "        log_jacobian_det = 0\n",
    "\n",
    "        x1_new = x1 + 0.\n",
    "        x2_new = x2 + 0.\n",
    "\n",
    "        for i in range(x1.size(0)):\n",
    "            x1_new[i] = x1[i] if c[0][i] > 0 else x2[i]\n",
    "            x2_new[i] = x2[i] if c[0][i] > 0 else x1[i]\n",
    "\n",
    "        return (x1_new, x2_new), log_jacobian_det\n",
    "    \n",
    "    def output_dims(self, dims_in):\n",
    "        return dims_in\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 10., -11.],\n",
      "        [  3., -17.]])\n",
      "tensor([[ 20., -22.],\n",
      "        [  6., -34.]])\n",
      "tensor([[ 10., -11.],\n",
      "        [  3., -17.]])\n",
      "tensor(-1.3863)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5116/1812892363.py:9: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x = torch.randn(batch_size, in_dimension) * 10 // 1\n"
     ]
    }
   ],
   "source": [
    "# 基础使用\n",
    "batch_size = 2\n",
    "in_dimension = 2\n",
    "\n",
    "net = FF.SequenceINN(in_dimension)\n",
    "for i in range(2):\n",
    "    net.append(FixedRandomElementwiseMultiply)\n",
    "\n",
    "x = torch.randn(batch_size, in_dimension) * 10 // 1\n",
    "\n",
    "z, det = net(x)\n",
    "\n",
    "x_rev, det_rev = net(z, rev=True) \n",
    "\n",
    "print(x)\n",
    "print(z)\n",
    "print(x_rev)\n",
    "print(det_rev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1., 11.],\n",
      "        [ 0., -2.]])\n",
      "tensor([[-1., 11.],\n",
      "        [ 0., -2.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5116/1314869716.py:20: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x1 = torch.randn(batch_size, in_dimension) * 10 // 1\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5116/1314869716.py:21: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x2 = torch.randn(batch_size, in_dimension) * 10 // 1\n",
      "C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_5116/1314869716.py:22: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  c = torch.randn(batch_size) // 1\n"
     ]
    }
   ],
   "source": [
    "# 复杂使用\n",
    "input1 = FF.InputNode(in_dimension, name='Input1')\n",
    "input2 = FF.InputNode(in_dimension, name='Input2')\n",
    "\n",
    "cond = FF.ConditionNode(1, name='ConditionNode')\n",
    "\n",
    "mult1 = FF.Node(input1.out0, FixedRandomElementwiseMultiply, {}, name='mult1')\n",
    "cond_swap = FF.Node([mult1.out0, input2.out0], ConditionalSwap, {}, conditions=cond, name='cond_swap')\n",
    "mult2 = FF.Node(cond_swap.out1, FixedRandomElementwiseMultiply, {}, name='mult2')\n",
    "\n",
    "output1 = FF.OutputNode(cond_swap.out0, name='output1')\n",
    "output2 = FF.OutputNode(mult2.out0, name='output2')\n",
    "\n",
    "inn_net = FF.GraphINN([\n",
    "    input1, input2, cond,\n",
    "    mult1, cond_swap, mult2,\n",
    "    output1, output2\n",
    "])\n",
    "\n",
    "x1 = torch.randn(batch_size, in_dimension) * 10 // 1\n",
    "x2 = torch.randn(batch_size, in_dimension) * 10 // 1\n",
    "c = torch.randn(batch_size) // 1\n",
    "\n",
    "(z1, z2), det = inn_net([x1, x2], c=c)\n",
    "(x1_rev, x2_rev), _ = inn_net([z1, z2], c=c,  rev=True, jac=False)\n",
    "\n",
    "print(x1)\n",
    "print(x1_rev)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
